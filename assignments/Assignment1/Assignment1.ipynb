{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning\n",
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "#### <span style=\"color:blue\">SUBMISSION INSTRUCTIONS WILL BE REALEASED SHORTLY</span>\n",
    "\n",
    "**It is important that you carefully follow the instructions below for things to work properly.**\n",
    "\n",
    "1. You need to have your environment set up as in the [README](https://github.com/amosstorkey/iaml-labs) and you need to activate this environment before running this notebook:\n",
    "```\n",
    "source activate py3iaml\n",
    "cd [DIRECTORY CONTAINING GIT REPOSITORY]\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Read the instructions carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the code/markdown cell where to put it\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (i.e. use the `datasets` directory **adjacent** to this file).\n",
    "\n",
    "1. Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation.\n",
    "\n",
    "1. Make sure to distinguish between **attributes** (columns of the data) and **features** (which typically refers only to the independent variables, i.e. excluding the target variables).\n",
    "\n",
    "1. Make sure to show **all** your code/working. \n",
    "\n",
    "1. Write readable code. While we do not expect you to follow [PEP8](https://www.python.org/dev/peps/pep-0008/) to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. **Do** use inline comments when doing something non-standard. When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer.\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "\n",
    "This assignment is formative and such will not count towards your final grade. Nonetheless, we ask you to submit answers to certain questions so that you can become familiar with the Gradescope system and so that we can summarize common mistakes people might make.\n",
    "\n",
    "We will be using [Gradescope](https://www.gradescope.com/) for submissions. Submission instructions will be released separately shortly. You will be using a separate Latex-based file where you would copy your answers and/or code.\n",
    "\n",
    "**IMPORTANT: Only specific questions need to be submitted. These are Question 2.2, Question 2.4, Question 4.3 and Question 4.4,**\n",
    "\n",
    "The submission deadline for this assignment by **Monday 14/10/2019 at 16:00**. \n",
    "\n",
    "Since this assignment is formative, there will be no marking assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Execute the cell below to import all packages you will be using in the rest of the assignemnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lara/miniconda3/envs/py3iaml/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils.plotter import scatter_jitter, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## 20 Newsgroup Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the dataset\n",
    "\n",
    "This question is based on the 20 Newsgroups Dataset. This dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware, comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale, soc.religion.christian). \n",
    "\n",
    "There are three versions of the 20 Newsgroups Dataset. In this assignment we will use the `bydate` matlab version in which documents are sorted by date into training (60%) and test (40%) sets, newsgroup-identifying headers are dropped and duplicates are removed. This collection comprises roughly 61,000 different words, which results in a bag-of-words representation with frequency counts. More specifically, each document is represented by a 61,000 dimensional vector that contains the counts for each of the 61,000 different words present in the respective document. \n",
    "\n",
    "To save you time and to make the problem manageable with limited computational resources, we preprocessed the original dataset. We will use documents from only 5 out of the 20 newsgroups, which results in a 5-class problem. The class is conveniently stored in the `class` column. More specifically the 5 classes correspond to the following newsgroups: \n",
    "1. `alt.atheism`\n",
    "2. `comp.sys.ibm.pc.hardware`\n",
    "3. `comp.sys.mac.hardware`\n",
    "4. `rec.sport.baseball`\n",
    "5. `rec.sport.hockey `\n",
    "\n",
    "However, note here that classes 2-3 and 4-5 are rather closely related. Additionally, we computed the [mutual information](https://en.wikipedia.org/wiki/Mutual_information) of each word with the class attribute and selected the some words out of 61,000 that had highest mutual information. For very sophisticated technical reasons (which you should know!) 1 was added to all the word counts in part 1. The resulting representation is much more compact and can be used directly to perform our experiments in Python.\n",
    "\n",
    "**Hint**: The data was preprocessed by a very busy PhD student... and hence should never be taken to be perfect at face value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to get a feel for the data that you will be dealing with in the rest of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 ==========\n",
    "\n",
    "1. [Code] Load the dataset `raw_20news.csv` into a data-frame called `news_raw`. Using pandas methods we learnt in class, extract some basic information about the data. \n",
    "\n",
    "1. [Text] In a short paragraph, summarise the key features of the dataset. *Hint: Look at what we did in the labs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'raw_20news.csv')\n",
    "news_raw = pd.read_csv(data_path, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 ==========\n",
    "1. [Code] Display the names of some of the attributes in the training datset. \n",
    "1. [Text] Describe the output and comment (1 or 2 sentences) keeping in mind the selection procedure for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['w1_aaa', 'w2_pins', 'w3_kmr', 'w4_notion', 'w5_queens', 'w6_dwyer',\n",
       "       'w7_defenseman', 'w8_gld', 'w9_tocchet', 'w10_home'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) # Your Code goes here:\n",
    "news_raw.columns[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## Naive Bayes classification\n",
    "Now we want to fit a Gaussian Naive Bayes model to the cleaned dataset. You might want first to familiarise yourself with the [`GaussianNB`](http://scikit-learn.org/0.21/modules/generated/sklearn.naive_bayes.GaussianNB.html) class in `Sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 ==========\n",
    "\n",
    "Load the cleaned datasets `train_20news.csv` and `test_20news.csv` into pandas dataframes `news_train` and `news_test` respectively. Using pandas summary methods, confirm that the data is similar in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            w1_aaa      w2_pins       w3_kmr    w4_notion    w5_queens  \\\n",
      "count  2099.000000  2099.000000  2099.000000  2099.000000  2099.000000   \n",
      "mean      1.025727     1.017627     1.014769     1.008576     1.006193   \n",
      "std       0.280109     0.205208     0.172657     0.115206     0.078473   \n",
      "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "25%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "50%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "75%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "max       8.000000     7.000000     5.000000     4.000000     2.000000   \n",
      "\n",
      "          w6_dwyer  w7_defenseman       w8_gld   w9_tocchet     w10_home  \\\n",
      "count  2099.000000    2099.000000  2099.000000  2099.000000  2099.000000   \n",
      "mean      1.010481       1.026203     1.024297     1.012387     1.084326   \n",
      "std       0.134174       0.618880     0.274219     0.153879     0.491139   \n",
      "min       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
      "25%       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
      "50%       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
      "75%       1.000000       1.000000     1.000000     1.000000     1.000000   \n",
      "max       4.000000      28.000000     5.000000     4.000000    16.000000   \n",
      "\n",
      "          ...       w512_constantly  w513_generate  w514_definite  \\\n",
      "count     ...           2099.000000    2099.000000    2099.000000   \n",
      "mean      ...              4.574083       4.464983       4.533111   \n",
      "std       ...              2.283028       2.273922       2.329654   \n",
      "min       ...              1.000000       1.000000       1.000000   \n",
      "25%       ...              3.000000       2.000000       3.000000   \n",
      "50%       ...              5.000000       4.000000       5.000000   \n",
      "75%       ...              7.000000       6.000000       7.000000   \n",
      "max       ...              8.000000       8.000000       8.000000   \n",
      "\n",
      "        w515_lacks  w516_combination  w517_sitting  w518_surface  \\\n",
      "count  2099.000000       2099.000000   2099.000000   2099.000000   \n",
      "mean      4.557885          4.531682      4.514531      4.509290   \n",
      "std       2.292246          2.333558      2.259005      2.287548   \n",
      "min       1.000000          1.000000      1.000000      1.000000   \n",
      "25%       3.000000          2.000000      3.000000      2.000000   \n",
      "50%       5.000000          5.000000      4.000000      4.000000   \n",
      "75%       7.000000          7.000000      6.000000      7.000000   \n",
      "max       8.000000          8.000000      8.000000      8.000000   \n",
      "\n",
      "       w519_fashion     w520_sit        class  \n",
      "count   2099.000000  2099.000000  2099.000000  \n",
      "mean       4.521201     4.412577     3.091472  \n",
      "std        2.295995     2.296504     1.395628  \n",
      "min        1.000000     1.000000     1.000000  \n",
      "25%        3.000000     2.000000     2.000000  \n",
      "50%        5.000000     4.000000     3.000000  \n",
      "75%        6.500000     6.000000     4.000000  \n",
      "max        8.000000     8.000000     5.000000  \n",
      "\n",
      "[8 rows x 521 columns]\n",
      "           w1_aaa     w2_pins  w3_kmr  w4_notion   w5_queens    w6_dwyer  \\\n",
      "count  128.000000  128.000000   128.0      128.0  128.000000  128.000000   \n",
      "mean     1.007812    1.031250     1.0        1.0    1.007812    1.015625   \n",
      "std      0.088388    0.278847     0.0        0.0    0.088388    0.124507   \n",
      "min      1.000000    1.000000     1.0        1.0    1.000000    1.000000   \n",
      "25%      1.000000    1.000000     1.0        1.0    1.000000    1.000000   \n",
      "50%      1.000000    1.000000     1.0        1.0    1.000000    1.000000   \n",
      "75%      1.000000    1.000000     1.0        1.0    1.000000    1.000000   \n",
      "max      2.000000    4.000000     1.0        1.0    2.000000    2.000000   \n",
      "\n",
      "       w7_defenseman      w8_gld  w9_tocchet    w10_home     ...      \\\n",
      "count     128.000000  128.000000  128.000000  128.000000     ...       \n",
      "mean        1.007812    1.039062    1.015625    1.085938     ...       \n",
      "std         0.088388    0.291678    0.124507    0.281373     ...       \n",
      "min         1.000000    1.000000    1.000000    1.000000     ...       \n",
      "25%         1.000000    1.000000    1.000000    1.000000     ...       \n",
      "50%         1.000000    1.000000    1.000000    1.000000     ...       \n",
      "75%         1.000000    1.000000    1.000000    1.000000     ...       \n",
      "max         2.000000    4.000000    2.000000    2.000000     ...       \n",
      "\n",
      "       w512_constantly  w513_generate  w514_definite  w515_lacks  \\\n",
      "count       128.000000     128.000000     128.000000  128.000000   \n",
      "mean          4.757812       4.351562       4.593750    4.445312   \n",
      "std           2.201453       2.175706       2.438011    2.387001   \n",
      "min           1.000000       1.000000       1.000000    1.000000   \n",
      "25%           3.000000       3.000000       2.000000    2.000000   \n",
      "50%           5.000000       4.000000       5.000000    4.000000   \n",
      "75%           7.000000       6.000000       7.000000    7.000000   \n",
      "max           8.000000       8.000000       8.000000    8.000000   \n",
      "\n",
      "       w516_combination  w517_sitting  w518_surface  w519_fashion    w520_sit  \\\n",
      "count         128.00000    128.000000    128.000000    128.000000  128.000000   \n",
      "mean            4.53125      4.453125      4.687500      4.421875    4.531250   \n",
      "std             2.31373      2.383868      2.390104      2.285212    2.370878   \n",
      "min             1.00000      1.000000      1.000000      1.000000    1.000000   \n",
      "25%             2.75000      2.000000      2.750000      2.000000    2.000000   \n",
      "50%             4.00000      5.000000      5.000000      4.000000    5.000000   \n",
      "75%             6.25000      7.000000      7.000000      6.000000    7.000000   \n",
      "max             8.00000      8.000000      8.000000      8.000000    8.000000   \n",
      "\n",
      "            class  \n",
      "count  128.000000  \n",
      "mean     3.078125  \n",
      "std      1.400840  \n",
      "min      1.000000  \n",
      "25%      2.000000  \n",
      "50%      3.000000  \n",
      "75%      4.000000  \n",
      "max      5.000000  \n",
      "\n",
      "[8 rows x 521 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your Code goes here:\n",
    "data_path_train = os.path.join(os.getcwd(), 'datasets', 'train_20news.csv')\n",
    "news_train = pd.read_csv(data_path_train, delimiter = ',')\n",
    "\n",
    "data_path_test = os.path.join(os.getcwd(), 'datasets', 'test_20news.csv')\n",
    "news_test = pd.read_csv(data_path_test, delimiter = ',')\n",
    "\n",
    "print(news_train.describe())\n",
    "print(news_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 ==========\n",
    "#### <span style=\"color:blue\">SUBMIT ANSWERS TO THIS QUESTION</span>\n",
    "\n",
    "[Text] Answer (in brief) the following two questions:\n",
    "1. What is the assumption behing the Naive Bayes Model?\n",
    "1. What would be the main issue we would have to face if we didn't make this assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1/2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 ==========\n",
    "\n",
    "1. [Code] By using the `scatter_jitter` function, display a scatter plot of the features `w281_ico` and `w273_tek` for the **cleaned** dataset `news_train`. Set the jitter value to an appropriate value for visualisation. Label axes appropriately.\n",
    "1. [Text] What do you observe about these two features? Does this impact the validity of the Naive Bayes assumption? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 ==========\n",
    "1. [Text] What is a reasonable baseline against which to compare the classiffication performance? *Hint: What is the simplest classiffier you can think of?*. \n",
    "1. [Code] Estimate the baseline performance on the *training* data in terms of classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (2) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_2_5'></a>\n",
    "### ========== Question 2.5 ==========\n",
    "\n",
    "1. [Code] Fit a Gaussian Naive Bayes model to the cleaned dataset. \n",
    "\n",
    "1. [Code] Report the classification accuracy on the **training** dataset and plot a Confusion Matrix for the result (labelling the axes appropriately).\n",
    "\n",
    "1. [Text] Comment on the performance of the model. Is the accuracy a reasonable metric to use for this dataset?\n",
    "\n",
    "*Hint: You may make use of utility functions we provided, as well as an sklearn method for computing confusion matrices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (2) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.6 ==========\n",
    "#### <span style=\"color:blue\">SUBMIT ANSWERS TO THIS QUESTION</span>\n",
    "\n",
    "[Text] Comment on the confusion matrix from the previous question. Does it look like what you would have expected? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.7 ==========\n",
    "\n",
    "Now we want to evaluate the generalisation of the classifier on new (i.e. unseen data). \n",
    "\n",
    "1. [Code] Use the classifier you trained in Question [2.5](#question_2_5) (i.e. on the cleaned dataset) and test its performance on the test dataset. Display classification accuracy and plot a confusion matrix of the performance on the test data. \n",
    "\n",
    "1. [Code] Also, reevaluate the performance of the baseline on the test data.\n",
    "\n",
    "1. [Text] In a short paragraph (3-4 sentences) compare and comment on the results with (a) the training data and (b) the baseline (on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (2) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.8 ==========\n",
    "1. [Code] Fit a Gaussian Naive Bayes model to the original raw dataset (including the outliers) and test its performance on the **test** set. \n",
    "\n",
    "1. [Text] Comment on the output and explain why or why not cleaning affects the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.9 ==========\n",
    "\n",
    "In this exercise we have fitted a Gaussian Naive Bayes classifier to the data (i.e. the class conditional densities are Gaussians). However, this is not ideally suited to our dataset. Can you explain why this is so? what kind of Naive Bayes model would you employ to this kind of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_3'></a>\n",
    "# Question 3\n",
    "## Automobile Pricing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the dataset\n",
    "This assignment is based on the automobile pricing dataset. Our goal will be to predict the price of automobiles based on various attributes. This data set consists of three types of entities: \n",
    "\n",
    "1. The specification of an automobile in terms of various characteristics \n",
    "\n",
    "1. Assigned insurance risk rating \n",
    "   * this rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuaries call this process ”symboling”. A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe. \n",
    "\n",
    "1. Normalized losses in use as compared to other cars\n",
    "  * the third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year (avg_loss/car/year). \n",
    "\n",
    "\n",
    "To save you time and to make the problem manageable with limited computational resources, we preprocessed the original dataset. We removed any instances that had one or more missing values and randomized the data set. The resulting representation is much more compact and can be used directly to perform our experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping into our problem, it is beneficial to get a feel for the data we are dealing with in the rest of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_3_1'></a>\n",
    "### ========== Question 3.1 ==========\n",
    "\n",
    "Load the dataset `train_auto_numeric.csv` into a pandas DataFrame called `auto_numeric`. Using any suitable pandas functionality, \n",
    "1. [Code] summarise *and*\n",
    "1. [Text] comment upon\n",
    "\n",
    "the key features of the data. Show all your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.2 ==========\n",
    "\n",
    "We will now examine the attributes in some detail. Familiarise yourself with the concept of Correlation Coefficients (start from the Lecture on Generalisation and Evaluation).\n",
    "\n",
    "1. [Code] Analyse first the relationship between each attribute and price:\n",
    "  1. Compute the correlation coefficient between each attribute and price, *and*\n",
    "  1. Visualise the (pairwise) distribution of each attribute with price\n",
    "1. [Text] Given the above, which attributes do you feel may be most useful in predicting the price? (mention at least 5). How did you reach this conclusion? *Hint: which is the more useful of the above tools?*\n",
    "1. [Code] Now we will analyse the relationship between the attributes themselves. Use an appropriate pairwise visualisation tool to display graphically the relationship between each pair of attributes you selected in (2).\n",
    "1. [Text] Do any attributes exhibit significant correlations between one-another? (restrict your analysis to useful attributes identified above)\n",
    "1. [Text] Which attributes (give examples) would you consider removing if we wish to reduce the dimensionality of the problem and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "## Multivariate Linear Regression\n",
    "In this Section we will fit a Multivariate Linear Regression model (still using [`LinearRegression`](http://scikit-learn.org/0.21/modules/generated/sklearn.linear_model.LinearRegression.html)) to the dataset: i.e. we will now train a model with **multiple** explanatory variables and ascertain how they affect our ability to predict the retail price of a car. \n",
    "\n",
    "**N.B. In this question we will perform K-fold cross-validation using scikit's *KFold* class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_4_1'></a>\n",
    "### ========== Question 4.1  ==========\n",
    "\n",
    "K-fold cross-validation.\n",
    "\n",
    "1. [Text] What other technique for validation could we use (rather than K-Fold cross-validation)?\n",
    "\n",
    "1. [Text] Given the analysis you did on the automobile dataset in [Question 3](#question_3), what problem are we trying to solve by using K-Fold cross-validation?\n",
    "\n",
    "1. [Code] To solve this problem, we will use k-fold cross-validation to evaluate the performance of the regression model. By using Scikit-learn's [`KFold`](http://scikit-learn.org/0.19/modules/generated/sklearn.model_selection.KFold.html) class construct a 5-fold cross-validation object. Set `shuffle=True` and `random_state=0`. ***[Optional]*** *You may wish to visualise the training/validation indices per fold. The `split` method comes in handy in this case.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.1  ==========\n",
    "\n",
    "1. [Code] Train a Multi-Variate `LinearRegression` model on the original `auto_numeric` dataframe you loaded in [Question 3.1](#question_3_1), and evaluate it using the *KFold* instance you created in [Question 4.1](#question_4_1) (report RMSE and $R^2$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.2 ==========\n",
    "\n",
    "1. [Code] Examine the scatter plot of `engine-size` vs `price` (plot below)\n",
    "1. [Text] Why might this cause a problem for linear regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_4_3'></a>\n",
    "### ========== Question 4.3 ==========\n",
    "#### <span style=\"color:blue\">SUBMIT ANSWERS TO THIS QUESTION</span>\n",
    "\n",
    "In class we discussed ways of preprocessing features to improve performance in such cases.\n",
    "1. [Code] Transform the `engine-size` attribute using an appropriate technique from the lectures (document it in your code) and show the transformed data (scatter plot).\n",
    "1. [Code] Then retrain a (Multi-variate) LinearRegression Model (on all the attributes including the transformed `engine-size`) and report $R^2$ and RMSE. \n",
    "1. [Text] How has the performance of the model changed when compared to the previous result? and why so significantly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (2) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.4 ==========\n",
    "\n",
    "#### <span style=\"color:blue\">SUBMIT ANSWERS TO THIS QUESTION</span>\n",
    "\n",
    "The simplicity of Linear Regression allows us to interpret the importance of certain features in predicting target variables. However this is not as straightforward as just reading off the coefficients of each of the attributes and ranking them in order of magnitude.\n",
    "\n",
    "1. [Text] Why is this? How can we *linearly* preprocess the attributes to allow for a comparison? Justify your answer.\n",
    "1. [Code] Perform the preprocessing you just mentioned on the transformed data-set from [Question 4.3](#question_4_3), retrain the Linear-Regressor and report the coefficients in a readable manner. *Tip: To simplify matters, you may abuse standard practice and train the model once on the entire data-set with no validation/test set.*\n",
    "1. [Text] Which are the three (3) most important features for predicting price under this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (2) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.5 ==========\n",
    "\n",
    "In the lectures we discussed another form of extension to the basic linear-regression model: the introduction of basis functions. This method attempts to capture non-linearities in the input-output mapping.\n",
    "\n",
    "1. [Text] How would you choose the features to test higher-orders on? And how would you choose the best polynomial order for these features?\n",
    "1. [Code] Load the csv file `train_auto_nonlinear.csv` into a new dataframe (this is a standard version of the transformed data-set from [Question 3.3](#question_3_3)). Add a second-order basis to the two attributes `length` and `engine-power` and train a new LinearRegression model. Report the $R^2$ and RMSE performance.\n",
    "1. [Text] Comment on the result in relation to those in [Question 4.3](#question_4_3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (2) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here:***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
